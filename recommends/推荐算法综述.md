# 推荐算法综述

近10年来，推荐、搜索和广告等业务发展迅速，被广泛应用于各个场景中。特别是移动互联网的兴起，大家对手机APP的使用体验要求越来越高，其中特别重要的体验就是个性化需求的体验。因此，个性化需求被应用到各个领域和场景，如电商方向的淘宝、京东、拼多多，或社区方向的今日头条、抖音APP等。近10年来，推荐、搜索和广告等业务发展迅速，被广泛应用于各个场景中。
本文将从推荐系统的整体框架出发，介绍推荐算法在推荐系统不同环节的应用。

![推荐算法综述_1](images/推荐算法综述_1.jpg)

上述是推荐系统的通用框架，推荐系统框架主要分为两个部分：召回，排序。在召回环节，通过不同的召回方式，如热门召回、兴趣召回、协调过滤召回等，从物品池中选取TopK的物品，组成个性化召回池；在排序环节，用统一的排序方法，将个人召回池中的所有召回物品进行排序，最后根据排序结果展现给用户。
所以，根据上述召回和排序环节的结构，我们要优化最终的个性化推荐效果，我们可以从这两个环节中的任一环节入手，尽准确的让用户快速浏览到物品。

以下将从召回和排序两个环节对推荐算法进行介绍。

## 1. 召回
在推荐系统框架中，召回的主要作用是从海量(十万以上即可)的物品中，快速找出用户可能有兴趣的物品。因为排序部分只是对召回的物品进行排序，所以用户有没有机会触达他喜欢的物品，在很大程度上取决于召回模块，如果物品连进召回的机会的没有，那么即使排序优化的再好，用户也没机会看到他喜欢的物品。

如上图所示，召回环节中，一般会通过多路召回方式并行的对物品进行召回，如热门召回、兴趣召回、协同过滤召回等，一般热门和兴趣是相对明确等，需要探索和扩展的主要是协同过滤部分，以下将介绍不同的协调过滤算法。

### 1.1 常用的协同过滤算法

* 近邻思想的协同过滤
    * 基于用户的协同过滤
    * 基于物品的协同过滤
* 矩阵分解的协同过滤
    * SVD 奇异值分解
    * ALS 交替最小二乘法
* 基于协同主体回归的协同过滤
    * 概率主题模型
    
### 1.2 近邻思想的协同过滤 一般应用于在线协同部分，要做各种剪枝来优化效果
基于近邻思想的协同过滤思想即为"相似的用户倾向于消费相似的物品"。因此，如何找到相似用户和相似物品是核心。相似度的度量方式有很多，如欧式距离、余弦距离、皮尔逊相关系数、相关性分析等。

欧式距离：
![推荐算法综述_2](images/推荐算法综述_2.png)

余弦距离：
![推荐算法综述_3](images/推荐算法综述_3.png)

皮尔逊相关系数：
![推荐算法综述_4](images/推荐算法综述_4.png)

#### 1.2.1 基于用户的协同过滤
基于用户的协同过滤（User-Based）方法是最早的一种[基于协同过滤的推荐算法](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.38.3784&rep=rep1&type=pdf)，是一种基于"用户可能会喜欢和他具有相似爱好的用户所喜欢的物品"的假设。换而言之，具体相似偏好的用户在物品打分上也应该相似的。
![推荐算法综述_5](images/推荐算法综述_5.png)

先通过相似度计算，选取与目标用户相似度最高的前 K 个用户，用这 K 个用户的打分情况对该目标用户对物品打分情况进行预测。最简单的基于用户的协同过滤推荐方法有两种：

* TopN 推荐：在这K哥用户中，统计出现频率最高且在目标用户的向量中未出现的物品，将这些物品构建成推荐列表。
* 关联推荐：利用这前 K 个用户的购买或打分记录进行关联规则挖掘，利用挖掘出的关联规则，结合目标用户的购买记录完成推荐，典型的推荐结果就是常见的“购买了某物品的用户还购买了什么物品”

当然，可以通过用户之间的相似度作为权重，加权得到用户 a 对物品 i 的打分。

基于用户的协同过滤方法：

* “个性化”的体现方式是：对于不同的用户，其最近邻是不同的，进而得到的推荐列表也不一样
* “协同过滤”的体现方式是：对一个用户进行推荐时，使用其它用户在物品上的观点来寻找和他相似的用户
* 优点在于：在数据集完善、物品丰富的条件下，能够获得较高的准确率， 而且能够避开物品信息上的挖掘进行推荐，能够对物品的关联性、用户的偏好进行隐式透明的挖掘
* 缺点在于：随着系统用户数量的增大，计算 Top-K 相关用户的时间会显著增长，使得该方法难以胜任用户量变化巨大的系统，*限制了系统的可扩展性*；同时，当新用户加入系统时，由于他的打分历史记录很少，难以准确计算真正与之对应的 Top-K 相关用户，这也进一步引出数据稀疏性的问题对系统可扩展性的限制。

#### 1.2.2 基于物品的协同过滤

鉴于基于用户的协同过滤方法，随着用户数增加，计算时间显著增长。Sarwar 等人在 WWW 2001 上提出了[基于物品的推荐](http://files.grouplens.org/papers/www10_sarwar.pdf)，基于物品的协同过滤方法是亚马逊网络商城的专利算法，目前也是亚马逊网络商城推荐系统的底层核心算法。

基于物品的协同过滤方法所基于的基本假设与基于用户的协同过滤方法类似，也就是“用户可能会喜欢与他之前曾经喜欢的物品相似的物品”。

![推荐算法综述_6](images/推荐算法综述_6.png)

最简单的线上推荐是，当用户购买了某一商品后，直接向其推荐与该物品相似度最高的前 K 个商品。稍微复杂一点的推荐方法考虑了该用户所有的打分历史记录：对于一个用户行向量中的 0 值(用户未购买的物品)，我们需要去预测该用户在该物品上可能的打分，预测方法是：考虑所有该用户已经打过分的物体，以它们与该物体的相似度为权重，对它们的分值进行加权平均，作为对该物体的预测打分，最终以预测打分的高低为顺序给出推荐列表

基于物品的协同过滤方法：

* 计算简单，容易实现实时响应，由于物品被打分的变化剧烈程度要比用户低得多，因此物品相似度的计算一般可以采用离线完成、定期更新的方式，从而减少了线上计算，实现实时响应，提高效率，尤其对于用户数远 大于商品数的情况下效果更加显著，比如用户新添加了几个感兴趣的商品之后，可以立即给出新的推荐
* 可解释性好，用户可能不了解其他人的购物情况，但是对自己的购物历史总是很清楚的，另外用户总是希望自己有最后的决定权，如果系统推荐的商品不满意，需要有办法让用户改进它，基于物品的协同过滤方法方法很容易让用户理解为什么推荐了某个商品，并且当用户在兴趣列表里添 加或删除商品时，可以调整系统的推荐结果

### 1.3 矩阵分解的协同过滤
基于矩阵分解的协同过滤方法实际上就是对用户-物品评分矩阵做一个分解，以得到用户和物品两个角度的潜在因素矩阵，最后可以得到这个矩阵中所有位置的近似解。该类模型可以是一种有监督的模型，所以可以离线统计衡量模型效果。该类型协同过滤算法应用相对较多，效果也不错。不过也需要进行一定的剪枝操作和热门打压操作。

#### 1.3.1 SVD分解
在推荐系统中，用户和物品之间没有直接关系。但是我们可以通过特征把它们联系在一起。对于电影来说，这样的特征可以是：喜剧还是悲剧，是动作片还是爱情片。用户和这样的特征之间是有关系的，比如某个用户喜欢看爱情片，另外一个用户喜欢看动作片；物品和特征之间也是有关系的，比如某个电影是喜剧，某个电影是悲剧。那么通过和特征之间的联系，我们就找到了用户和物品之间的关联。

SVD 奇异值分解在数学上的定义指的是，对于任意一个 m 行 n 列的矩阵 $A_{m*n}$，可以被写成三个矩阵的乘积：

U：m 行 m 列的列正交矩阵，用来表示用户层面的特征
S：m 行 n 列的对角线矩阵，矩阵元素非负
V：n 行 n 列的正交矩阵的倒置，用来表示物品层面的特征
即：
![推荐算法综述_7](images/推荐算法综述_7.png)

由于 S 是一个对角矩阵，每个元素非负，而且依次减小。可以大致理解如下：在线性空间里，每个向量代表一个方向。所以特征值是代表该矩阵向着该特征值对应特征向量方向的变化权重。所以我们可以取 S 对角线上前 k 个元素，这样 U 和 V 也会相应的降维，从而实现了降维的目的，减少计算量。

这样，当我们得到一个新用户对物品的评分向量后，可通过与 U 和 S 矩阵的乘积，得到新用户的特征表示，从而找到与其相近的其他用户，再基于这些相近的用户对物品的打分情况，给出新用户对其未打分商品的评分预测。

但由于 SVD 奇异值分解计算量较大，所以一般都用 矩阵分解（Matrix Factorization）的形式
![推荐算法综述_8](images/推荐算法综述_8.png)

这两个矩阵 U 和 V 是通过学习的方式得到的，而不是直接做矩阵分解。可以通过定义损失函数，然后通过梯度下降的方法，逼近得到这两个矩阵，从而实现用户对物品评分的预测。

#### 1.3.2  ALS 交替最小二乘法
交替最小二乘法（Alternatingleast squares）实际上就是上述损失函数最小化的一个求解方法，当然还有其他方法比如 SGD 等。每次迭代，

* 固定 V，逐个更新每个 user 的特征 u (对 u 求偏导，令偏导为 0 求解)
* 固定 U，逐个更新每个 item 的特征 v (对 v 求偏导，令偏导为 0 求解)

#### 1.3.3 ALS-WR 模型

以上模型适用于用户对商品的有明确的评分矩阵的场景，然而很多情况下用户没有明确的反馈对商品的偏好，而是通过一些行为隐式的反馈。比如对商品的购买次数、对电视节目收看的次数或者时长，这时我们可以推测次数越多，看得时间越长，用户的偏好程度越高，但是对于没有购买或者收看的节目，可能是由于用户不知道有该商品，或者没有途径获取该商品，我们不能确定的推测用户不喜欢该商品。ALS-WR 通过置信度的权重来解决此问题，对于我们更确信用户偏好的项赋予较大的权重，对于没有反馈的项，赋予较小的权重。目标函数如下：

![推荐算法综述_9](images/推荐算法综述_9.png)

在实际应用中，由于待分解的矩阵常常是非常稀疏的，与 SVD 相比，ALS 能有效的解决过拟合问题。基于 ALS 的矩阵分解的协同过滤算法的可扩展性也优于 SVD。与随机梯度下降的求解方式相比，一般情况下随机梯度下降比 ALS 速度快。但有两种情况 ALS 更优于随机梯度下降：当系统能够并行化时，ALS 的扩展性优于随机梯度下降法。ALS-WR 能够有效的处理用户对商品的隐式反馈的数据ALS-WR 模型

以上模型适用于用户对商品的有明确的评分矩阵的场景，然而很多情况下用户没有明确的反馈对商品的偏好，而是通过一些行为隐式的反馈。比如对商品的购买次数、对电视节目收看的次数或者时长，这时我们可以推测次数越多，看得时间越长，用户的偏好程度越高，但是对于没有购买或者收看的节目，可能是由于用户不知道有该商品，或者没有途径获取该商品，我们不能确定的推测用户不喜欢该商品。ALS-WR 通过置信度的权重来解决此问题，对于我们更确信用户偏好的项赋予较大的权重，对于没有反馈的项，赋予较小的权重。目标函数如下：在实际应用中，由于待分解的矩阵常常是非常稀疏的，与 SVD 相比，ALS 能有效的解决过拟合问题。基于 ALS 的矩阵分解的协同过滤算法的可扩展性也优于 SVD。与随机梯度下降的求解方式相比，一般情况下随机梯度下降比 ALS 速度快。但有两种情况 ALS 更优于随机梯度下降：

* 当系统能够并行化时，ALS 的扩展性优于随机梯度下降法
* ALS-WR 能够有效的处理用户对商品的隐式反馈的数据

ALS算法在Netflix的一次比赛中获得了第一的名次，另外spark-ml也集成了ALS和ALS-WR算法。

### 1.4 协同主题回归算法

协同主题回归算法实际上就是结合主题概率模型和协同过滤模型。最简单的结合思路是：

* 将主题分布概率 $\theta$ 替代协同过滤中物品的潜特征 $v$，即
![推荐算法综述_10](images/推荐算法综述_10.png)

* 但这种方式会导致 内容有差异但主题相似 的文章容易被认作相似，而得到推荐

因此，协同主题回归算法的结合思路是：
![推荐算法综述_11](images/推荐算法综述_11.png)
协同主题回归算法可以在一定程度上解决 冷启动 的问题，通过概率主题模型作为补充，更好地去解决矩阵分解过程中 out-matrix 的现象。

## 2. 排序

排序模型主要目的就是将召回的个性化物品进行进一步的优化排序，从而提高用户的浏览效率。一般情况下，排序模型都是有监督的模型，通过有监督的方法优化排序后用户浏览行为的某一指标，如点击率、浏览时长等，用分类或回归的方式拟合用户对某个物品的行为指标，如点击率，从个人将预测为高点击率的物品排在前面，使得用户能快速触达。
常用的排序模型主要分为两种：点击率预估模型（CTR）和Learning To Rank模型（LTR），CTR主要应用在feed场景下，LTR主要应用在搜索排序场景下。

### 2.1 常见的排序模型

* 传统分类模型
    * LR
    * FM
* Boost模型
    * GBDT
    * Xgboost
    * 其他Boost模型
* 深度模型
    * DNN
    * Wide and Deep 
    * DeepFM
* 增强学习 reinforcement learning
    * Q-learning
    * reinforcement learning

### 2.2 传统分类模型
传统的分类和回归模型有很多，而适用于推荐和广告系统的不多，主要为LR和FM两个模型。

#### 2.2.1 Logistic Regression, LR模型
点击率预估：把被点击的样本当成正例，把未点击的样本当成负例，那么样本的ctr实际上就是样本为正例的概率。因此只要有分类算法可以算出样本为正的概率，就可以作为点击率预估模型使用，而LR就满足这样的条件。另外LR相比于其他模型有求解简单、可解释强的优点，这也是工业界所看重的。

Logistic Regression的函数如下所示：
![推荐算法综述_12](images/推荐算法综述_12.jpeg)

假设我们已经训练好了一组权值¥$w_{}^T$。只要把我们需要预测的 $x$ 代入到上面的方程，输出的y值就是这个标签为正类的概率，我们就能够判断输入数据是属于哪个类别。其中w即为权值参数，x为样本特征。
LR的损失函数如下所示：
![推荐算法综述_13](images/推荐算法综述_13.jpeg)

根据上述的损失函数定义，然后通过梯度下降的方法，逼近得到权值参数w，得到最早函数结果。

需要单独强调的是，在Logistic Regression 模型中，权值参数w和样本特征x之间是线性关系。在这里x表示什么呢？一般情况下x有三部分组成：用户特征、物品特征、场景特征，这三种特征共同构成了x的特征空间。很多时候，用户特征和物品特征相关连的时候，才和点击行为有强相关性，那么这种关联性在LR模型中是没办法体现的。对于这种情况，一般会有两种方式解决：

* 人工构建高阶特征。在进入模型之前，可以用笛卡尔积的方式人工对两两相关的特征进行交叉，从而得到相应的二阶、甚至高阶特征，然后这些特征作为一种x的组成，加入到模型的特征空间中；
* 人工构建特征成本太高，通过优化模型，在模型中解决特征的交叉问题。

下面的FM和其他树模型，就具备自己构建二阶甚至高阶特征的能力。

#### 2.2.2 [Factorization Machines](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf), FM模型
和Logistic Regression模型相比，FM模型增加了二阶特征项，模型具体如下所示：

![推荐算法综述_14](images/推荐算法综述_14.png)

通过上述公式可知，FM比LR模型多了二阶项，并且这个二阶项和直接将x两两相乘不太，它的表示含义中存在着因子积分解的理念。
![推荐算法综述_15](images/推荐算法综述_15.png)

通过上诉可知，经过公式变化，这种n^2的复杂度计算最后可以等价为n*k的复杂度，大大降低了计算量。

和LR相同的，FM可以构造类似的损失函数，然后用梯度下架的方法，从而训练得到参数w、v。

相对LR而言，模型FM增加了特征两两之间的关联信息，并且还将模型复杂度控制住O(n)，大大提高了模型的表现能力。不过他依然存在不少弊端：

* 超参数k的选取，k越大，模型表现力越强，但计算复杂度越高，一个合适的k显得非常重要；
* 模型表达了二阶的相关性，但是不能引入更高阶的信息；

在上述两个模型中，如果希望模型最后能较好的进行点击率预测，往往都逃不开人工特征工程。

### 2.3 Boosting
Boosting方法是一种用来提高弱分类算法准确度的方法,这种方法通过构造一个预测函数系列,然后以一定的方式将他们组合成一个预测函数。 Boosting是一种提高任意给定学习算法准确度的方法。下面主要介绍树状结构的Boosting模型——Xgboost和线性结构LR的Boosting模型-MLR。

#### 2.3.1 [Xgboost](https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf) 模型
Xgboost模型

#### 2.3.2 [大规模稀疏非线性模型 MLR](https://arxiv.org/pdf/1704.05194.pdf)

### 2.4 深度模型

#### 2.4.1 [DNN 模型](https://arxiv.org/pdf/1601.02376.pdf)

#### 2.4.2 [Wide And Deep](https://arxiv.org/pdf/1606.07792.pdf)

#### 2.4.3 [DeepFM](https://arxiv.org/pdf/1703.04247.pdf)

### 2.5 增强学习

#### 2.5.1 Q-Learning

#### 2.5.2 Reinforcement Learning

## 参考文献
* [A survey of collaborative filtering techniques](http://downloads.hindawi.com/archive/2009/421425.pdf)
* [Matrix Factorization Techniques for Recommender Systems](https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf)
* [Factorization machines](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)
* [XGBoost: A Scalable Tree Boosting System](https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf)
* [Deep Learning over Multi-field Categorical Data](https://arxiv.org/pdf/1601.02376.pdf)
* [Wide & deep learning for recommender systems](https://arxiv.org/pdf/1606.07792.pdf)
* [DeepFM: A Factorization-Machine based Neural Network for CTR Prediction](https://arxiv.org/pdf/1703.04247.pdf)
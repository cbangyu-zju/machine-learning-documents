# 卷积神经网络之AlexNet
AlexNet，由Khrizhevsky等人在2012年的ImageNet图像识别挑战赛中提出，且一举夺冠，识别错误率远超第二名大概10个百分点。和早年的LeNet相比，AlexNet是一个8层网络，其中有五层卷积层和2层全连接层，以及一个输出层。另外为了解决因网络层数增加可能导致的性能低、过拟合情况严重等问题，AlexNet提出了几个解决方式：1. ReLu的激活函数，2. Dropout网络结构，3. Data augmentation，4. 多GPU并行，5. 局部响应归一化，等等。

![图AlexNet_1](images/AlexNet_1)
![图AlexNet_2](images/AlexNet_2)

AlexNet的网络结构示意图如上所示，其中图1为论文中的网络结构，看起来比较复杂，可以简化为Ng课程中的示意图。

* Conv_1：卷积层，输入：227\*227\*3，使用96个11\*11\*3的卷积核，步长为4，卷积之后长宽为 (227-11)/4 + 1 = 55，所以输出为55\*55\*96
* Max-pooling：输入：55\*55\*96，pool：3\*3，步长为：2，所以pool之后长宽为：(55-3)/2 + 1 = 27，所以输出为：27\*27\*96
* Conv_2：卷积层，输入：27\*27\*96，使用256个5\*5\*96的卷积核，步长为1，padding为same。Padding常用的有两种：same，valid。因为此时有padding且步长为1，因此输出层为27\*27\*256
* Max-pooling：输出：27\*27\*256，pool为3*3，步长为2，长度为(27-3)/2 + 1 = 13，输出层为：13\*13\*256
* Conv_3,Conv_4,Conv_5:输入输出均为{13 * 13 * 256},步长为{1},核为{3 * 3 * 256},Padding为Same
* Max-pooling:输入{13 * 13 * 256}，pool:{3 * 3},步长为{2}输出结果为{6 * 6 * 256},也就是{9216}个特征变量
* FC:全连接层有两个隐藏层，从9216 =>4096 => 4096 => 1000(softmax)

AlexNet网络结果如上。AlexNet模型除了网络结构有较大变化外，还做了很多其他的尝试，主要有：
1. 非线性激活函数：ReLU；2. 防止过拟合的方法：Dropout，Data augmentation；3. 大数据训练：百万级ImageNet图像数据；4. 其他：GPU实现，LRN归一化层的使用

* ReLU
	* 优点：
		* ReLU本质上是分段线性模型，前向计算非常简单，无需指数之类操作；
		* ReLU的偏导也很简单，反向传播梯度，无需指数或者除法之类操作；
		* ReLU不容易发生梯度发散问题，Tanh和Logistic激活函数在两端的时候导数容易趋近于零，多级连乘后梯度更加约等于0；
		* ReLU关闭了右边，从而会使得很多的隐层输出为0，即网络变得稀疏，起到了类似L1的正则化作用，可以在一定程度上缓解过拟合。
	* 缺点：
		* 左边全部关了很容易导致某些隐藏节点永无翻身之日，所以后来又出现pReLU、random ReLU等改进，而且ReLU会很容易改变数据的分布，因此ReLU后加Batch Normalization也是常用的改进的方法。
* Data Augmentation
	* 从原始图像（256,256）中，随机的crop出一些图像（224,224）。【平移变换，crop】
	* 水平翻转图像。【反射变换，flip】
	* 给图像增加一些随机的光照。【光照、彩色变换，color jittering】
	* AlexNet 训练的时候，在data augmentation上的处理：
		* 随机crop。训练时候，对于256＊256的图片进行随机crop到224＊224，然后允许水平翻转，那么相当与将样本倍增到{((256-224)^2)*2=2048}。
		* 测试时候，对左上、右上、左下、右下、中间做了5次crop，然后翻转，共10个crop，之后对结果求平均。作者说，不做随机crop，大网络基本都过拟合(under substantial overfitting)。
		* 对RGB空间做PCA，然后对主成分做一个{(0, 0.1)}的高斯扰动。结果让错误率又下降了1%。
* Dropout
	* 相当于一种模型集成。 每个隐藏层神经元的输入以0.5的概率输出为0。输出为0的神经元相当于从网络中去除，不参与前向计算和反向传播。所以对于每次输入，神经网络都会使用不同的结构。另外Dropout甚至起到了boosting的作用。
* Local Response Normalization
	* 局部响应归一化原理是仿造生物学上活跃的神经元对相邻神经元的抑制现象(本质上，这一层也是为了防止激活函数的饱和)
* 多GPU训练
	* 由于早期GPU显存的限制，AlexNet使用了双数据流的设计，以让网络中一半的节点能存入一个GPU。这两个数据流，也就是说两个GPU只在一部分层进行通信，这样达到限制GPU同步时的额外开销的效果。有幸的是，GPU在过去几年得到了长足的发展，除了一些特殊的结构外，我们也就不再需要这样的特别设计了。
